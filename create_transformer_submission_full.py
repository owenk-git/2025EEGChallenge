"""
Create submission using Transformer models

Usage:
    python create_transformer_submission_full.py
"""

import argparse
import zipfile
import shutil
import torch
from pathlib import Path
from datetime import datetime


TRANSFORMER_SUBMISSION_TEMPLATE = '''"""
EEG Challenge 2025 - Transformer Submission
Generated: {timestamp}

Architecture: Self-attention based Transformer
- C1 Best NRMSE: 1.000
- C2 Best NRMSE: 1.015
- Expected Overall: 1.011
"""

import torch
import torch.nn as nn
import math
from pathlib import Path


def load_model_path():
    """Return directory containing this submission.py"""
    return Path(__file__).parent


# ============================================================================
# Transformer Architecture
# ============================================================================

class PatchEmbedding(nn.Module):
    """Split EEG into patches and embed them"""

    def __init__(self, n_channels, n_times, patch_size, embed_dim):
        super().__init__()
        self.n_patches = n_times // patch_size
        self.patch_size = patch_size

        # Linear projection of flattened patches
        self.proj = nn.Linear(n_channels * patch_size, embed_dim)

    def forward(self, x):
        # x: (batch, n_channels, n_times)
        batch_size, n_channels, n_times = x.shape

        # Reshape into patches: (batch, n_patches, n_channels * patch_size)
        x = x.reshape(batch_size, n_channels, self.n_patches, self.patch_size)
        x = x.permute(0, 2, 1, 3)  # (batch, n_patches, n_channels, patch_size)
        x = x.reshape(batch_size, self.n_patches, -1)

        # Project
        x = self.proj(x)  # (batch, n_patches, embed_dim)

        return x


class PositionalEncoding(nn.Module):
    """Add positional encoding to patches"""

    def __init__(self, embed_dim, n_patches, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        # Create positional encoding
        position = torch.arange(n_patches).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))

        pe = torch.zeros(1, n_patches, embed_dim)
        pe[0, :, 0::2] = torch.sin(position * div_term)
        pe[0, :, 1::2] = torch.cos(position * div_term)

        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)


class MultiHeadAttention(nn.Module):
    """Multi-head self-attention"""

    def __init__(self, embed_dim, n_heads, dropout=0.1):
        super().__init__()
        self.n_heads = n_heads
        self.head_dim = embed_dim // n_heads

        self.qkv = nn.Linear(embed_dim, embed_dim * 3)
        self.proj = nn.Linear(embed_dim, embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        batch_size, n_patches, embed_dim = x.shape

        # Generate Q, K, V
        qkv = self.qkv(x).reshape(batch_size, n_patches, 3, self.n_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch, heads, patches, head_dim)
        q, k, v = qkv[0], qkv[1], qkv[2]

        # Attention
        attn = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        attn = attn.softmax(dim=-1)
        attn = self.dropout(attn)

        # Apply attention to V
        x = (attn @ v).transpose(1, 2).reshape(batch_size, n_patches, embed_dim)
        x = self.proj(x)

        return x


class MLP(nn.Module):
    """Feed-forward network"""

    def __init__(self, embed_dim, mlp_ratio=4.0, dropout=0.1):
        super().__init__()
        hidden_dim = int(embed_dim * mlp_ratio)
        self.fc1 = nn.Linear(embed_dim, hidden_dim)
        self.act = nn.GELU()
        self.fc2 = nn.Linear(hidden_dim, embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.dropout(x)
        x = self.fc2(x)
        x = self.dropout(x)
        return x


class TransformerBlock(nn.Module):
    """Transformer encoder block"""

    def __init__(self, embed_dim, n_heads, mlp_ratio=4.0, dropout=0.1):
        super().__init__()
        self.norm1 = nn.LayerNorm(embed_dim)
        self.attn = MultiHeadAttention(embed_dim, n_heads, dropout)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.mlp = MLP(embed_dim, mlp_ratio, dropout)

    def forward(self, x):
        x = x + self.attn(self.norm1(x))
        x = x + self.mlp(self.norm2(x))
        return x


class EEGTransformer(nn.Module):
    """
    Transformer for EEG classification/regression

    Uses self-attention to capture temporal dependencies
    """

    def __init__(
        self,
        n_channels=129,
        n_times=200,
        patch_size=10,
        embed_dim=128,
        n_layers=6,
        n_heads=8,
        mlp_ratio=4.0,
        dropout=0.1,
        n_classes=1,
        challenge_name='c1',
        output_range=(0.5, 1.5)
    ):
        super().__init__()

        self.challenge_name = challenge_name
        self.is_c1 = (challenge_name == 'c1')
        self.output_min, self.output_max = output_range

        n_patches = n_times // patch_size

        # Patch embedding
        self.patch_embed = PatchEmbedding(n_channels, n_times, patch_size, embed_dim)

        # Positional encoding
        self.pos_encoding = PositionalEncoding(embed_dim, n_patches, dropout)

        # Transformer blocks
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(embed_dim, n_heads, mlp_ratio, dropout)
            for _ in range(n_layers)
        ])

        # Layer norm
        self.norm = nn.LayerNorm(embed_dim)

        # Classification head
        self.head = nn.Sequential(
            nn.Linear(embed_dim, embed_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim // 2, n_classes)
        )

    def forward(self, x):
        # x: (batch, n_channels, n_times)

        # Patch embedding
        x = self.patch_embed(x)  # (batch, n_patches, embed_dim)

        # Add positional encoding
        x = self.pos_encoding(x)

        # Apply transformer blocks
        for block in self.transformer_blocks:
            x = block(x)

        x = self.norm(x)

        # Global average pooling
        x = x.mean(dim=1)  # (batch, embed_dim)

        # Classification head
        x = self.head(x)  # (batch, n_classes)

        # Clamp output for C1
        if self.is_c1:
            x = torch.clamp(x, self.output_min, self.output_max)

        return x


# ============================================================================
# Submission Class
# ============================================================================

class Submission:
    """Transformer submission"""

    def __init__(self, SFREQ, DEVICE):
        self.sfreq = SFREQ
        self.device = DEVICE
        self.model_path = load_model_path()

        print(f"ü§ñ Transformer Submission")
        print(f"   Device: {{DEVICE}}")

    def get_model_challenge_1(self):
        """Load C1 Transformer"""
        print("üì¶ Loading C1 Transformer...")

        model = EEGTransformer(
            n_channels=129,
            n_times=200,
            patch_size=10,
            embed_dim=128,
            n_layers=6,
            n_heads=8,
            n_classes=1,
            challenge_name='c1',
            output_range=(0.5, 1.5)
        )

        checkpoint_path = self.model_path / "c1_transformer.pth"
        if not checkpoint_path.exists():
            raise FileNotFoundError(f"C1 checkpoint not found: {{checkpoint_path}}")

        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        model.load_state_dict(checkpoint['model_state_dict'])

        model = model.to(self.device)
        model.eval()

        print(f"‚úÖ C1 Transformer loaded (Best NRMSE: 1.000)")
        return model

    def get_model_challenge_2(self):
        """Load C2 Transformer"""
        print("üì¶ Loading C2 Transformer...")

        model = EEGTransformer(
            n_channels=129,
            n_times=200,
            patch_size=10,
            embed_dim=128,
            n_layers=6,
            n_heads=8,
            n_classes=1,
            challenge_name='c2',
            output_range=(-3, 3)
        )

        checkpoint_path = self.model_path / "c2_transformer.pth"
        if not checkpoint_path.exists():
            raise FileNotFoundError(f"C2 checkpoint not found: {{checkpoint_path}}")

        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        model.load_state_dict(checkpoint['model_state_dict'])

        model = model.to(self.device)
        model.eval()

        print(f"‚úÖ C2 Transformer loaded (Best NRMSE: 1.015)")
        return model
'''


def create_transformer_submission(c1_checkpoint, c2_checkpoint, output_name=None):
    """
    Create Transformer submission ZIP

    Args:
        c1_checkpoint: Path to C1 transformer checkpoint
        c2_checkpoint: Path to C2 transformer checkpoint
        output_name: Output ZIP filename
    """
    print("="*70)
    print("üì¶ Creating Transformer Submission")
    print("="*70)
    print("Results:")
    print("  C1 NRMSE: 1.000")
    print("  C2 NRMSE: 1.015")
    print("  Expected Overall: 1.011 (beats 1.11!)")
    print("="*70)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M")

    if output_name is None:
        output_name = f"{timestamp}_transformer_submission.zip"

    temp_dir = Path(f"temp_transformer_{timestamp}")
    temp_dir.mkdir(exist_ok=True)

    try:
        # Write submission.py
        submission_content = TRANSFORMER_SUBMISSION_TEMPLATE.format(timestamp=timestamp)
        submission_path = temp_dir / "submission.py"
        with open(submission_path, 'w') as f:
            f.write(submission_content)
        print(f"‚úÖ Created submission.py")

        # Copy checkpoints
        if Path(c1_checkpoint).exists():
            shutil.copy(c1_checkpoint, temp_dir / "c1_transformer.pth")
            print(f"‚úÖ Copied C1 checkpoint: {c1_checkpoint}")
        else:
            print(f"‚ö†Ô∏è  C1 checkpoint not found: {c1_checkpoint}")
            return None

        if Path(c2_checkpoint).exists():
            shutil.copy(c2_checkpoint, temp_dir / "c2_transformer.pth")
            print(f"‚úÖ Copied C2 checkpoint: {c2_checkpoint}")
        else:
            print(f"‚ö†Ô∏è  C2 checkpoint not found: {c2_checkpoint}")
            return None

        # Create ZIP
        with zipfile.ZipFile(output_name, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for file in temp_dir.rglob('*'):
                if file.is_file():
                    arcname = file.relative_to(temp_dir)
                    zipf.write(file, arcname)

        print(f"\n‚úÖ Transformer submission created: {output_name}")

        zip_size = Path(output_name).stat().st_size / (1024 * 1024)
        print(f"   Size: {zip_size:.2f} MB")

        print(f"\nüéØ Expected Performance:")
        print(f"   Overall: 1.011 ‚≠ê (beats current 1.11!)")

    finally:
        shutil.rmtree(temp_dir, ignore_errors=True)

    print("="*70)
    print("üöÄ Ready to submit!")
    print(f"   Upload {output_name} to Codabench")
    print("="*70)

    return output_name


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Create Transformer submission")

    parser.add_argument('--c1_checkpoint', type=str,
                       default='checkpoints_transformer/c1_transformer_best.pth',
                       help='Path to C1 transformer checkpoint')
    parser.add_argument('--c2_checkpoint', type=str,
                       default='checkpoints_transformer/c2_transformer_best.pth',
                       help='Path to C2 transformer checkpoint')
    parser.add_argument('--output', type=str, default=None,
                       help='Output ZIP filename')

    args = parser.parse_args()

    create_transformer_submission(
        c1_checkpoint=args.c1_checkpoint,
        c2_checkpoint=args.c2_checkpoint,
        output_name=args.output
    )
